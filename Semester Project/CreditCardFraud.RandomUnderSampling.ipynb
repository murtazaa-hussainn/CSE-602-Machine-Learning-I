{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Semester Project\n",
    "## Murtaza Hussain (29449) and Muhammad Asad ur Rehman (29456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Problem\n",
    "\n",
    "The below code solves the prevalent problem of imbalanced dataset, where one class dominates the dataset as compared to the other. Such is the case for the following dataset for Credit Card Transactions to detect Fraudulent Transactions. We will evaluate the following methods to resolve Class Imbalance:\n",
    "1. Random Under Sampling\n",
    "2. Algorithmic Methods (Using Random Forest as well as modifying Class Weights)\n",
    "3. Anomaly Detection Method\n",
    "\n",
    "For the following Dataset, we will use the following 5 Algorithms to draw a comparision between different methods:\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN)\n",
    "3. Random Forest\n",
    "4. Support Vector Machines (SVM)\n",
    "5. Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader loads data from CSV Files\n",
    "def load_dataset():\n",
    "    dataset = pd.read_csv(\"./Source.CreditCardFraud.csv\")\n",
    "    return dataset\n",
    "\n",
    "df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: Time\n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V1\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V2\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V3\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V4\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V5\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V6\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V7\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V8\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V9\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V10\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V11\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V12\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V13\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V14\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V15\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V16\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V17\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V18\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V19\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V20\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V21\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V22\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V23\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V24\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V25\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V26\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V27\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V28\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Amount\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Class\n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function performs a missing value analysis on each column of the dataset, helps you decide on what to do in cleaning process\n",
    "def null_check(df):\n",
    "    null_columns = []\n",
    "    for column in df.columns:\n",
    "        print(\"Column Name:\", column)\n",
    "        print(\"Column DataType:\", df[column].dtype)\n",
    "        if df[column].dtype != 'float64' and df[column].dtype != 'int64':\n",
    "            print(\"Column unique values:\", df[column].unique())\n",
    "        print(\"Column has null:\", df[column].isnull().any())\n",
    "\n",
    "        \n",
    "        if df[column].isnull().any() == True:\n",
    "            print(\"Column Null Count:\", df[column].isnull().sum())\n",
    "            null_columns.append(column)\n",
    "        print(\"\\n\")\n",
    "    return null_columns\n",
    "\n",
    "null_check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops any null columns and missing values\n",
    "# This is where you decide whether to remove NULL rows (which will reduce the size of Dataset) or remove NULL columns entirely. You can also choose a combination of both.\n",
    "def clean_data(df, drop_columns, missing_value = False):\n",
    "    # Remove unnecessary columns\n",
    "    df.drop(drop_columns, axis=1, inplace=True)\n",
    "    # Drop rows with any missing values\n",
    "    if missing_value == False:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        df.fillna(missing_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.78  99776\n",
      "1   0.22    223\n"
     ]
    }
   ],
   "source": [
    "# Prints a summary of class instances and distribution\n",
    "def data_summary(df, target=None):\n",
    "    if isinstance(df, pd.DataFrame) and target!=None:\n",
    "        a = df[target].value_counts()\n",
    "    else:\n",
    "        a = df.value_counts()\n",
    "    class0 = format(100 * a[0]/sum(a), \".2f\")\n",
    "    class1 = format(100 * a[1]/sum(a), \".2f\")\n",
    "\n",
    "    meta = pd.DataFrame([{ \"%\": class0, \"count\": a[0]},\n",
    "                         { \"%\": class1, \"count\": a[1]}])\n",
    "    print(\"\\nClass Distribution:\\n\", meta, \"\\n\")\n",
    "\n",
    "data_summary(df,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: Index(['Time', 'Class'], dtype='object')\n",
      "Numerical columns: Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
      "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
      "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Transforms categorical and numberical data into numerical data\n",
    "def transform_data(df):\n",
    "    # Encode categorical variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    print(\"Categorical columns:\", df.select_dtypes(include=['object', 'int64']).columns)\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Numerical columns:\", df.select_dtypes(include=['float64']).columns)\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    return df\n",
    "\n",
    "df['Class'] = df['Class'].astype(str)\n",
    "df = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for Baseline Run:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.78  99776\n",
      "1   0.22    223\n",
      "Logistic Regression Model Training Completed\n",
      "Random Forest Model Training Completed\n",
      "K-Nearest Neighbours Model Training Completed\n",
      "Support Vector Machines Model Training Completed\n",
      "Artificial Neural Networks Model Training Completed\n",
      "     Method                  Classifier  Class 1 Recall  Class 1 Precision\n",
      "0  Baseline         Logistic Regression          0.5648             0.7965\n",
      "1  Baseline               Random Forest          0.8383             0.9645\n",
      "2  Baseline        K-Nearest Neighbours          0.8298             0.9415\n",
      "3  Baseline     Support Vector Machines          0.7265             0.9697\n",
      "4  Baseline  Artificial Neural Networks          0.8385             0.9559\n"
     ]
    }
   ],
   "source": [
    "# Runs Baseline Model for All 5 Algorithms\n",
    "def BaselineRunAll(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    print(\"Class Distribution for Baseline Run:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression()\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC()\n",
    "    ann_classifier = MLPClassifier(max_iter=1000)\n",
    "    \n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)     # The reasoning behind k = 10 is so as to strike a balance between test and train samples of minority class\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), # As the majority class has 99.81% presence, accuracy cannot be used as a metric to evaluate performance\n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "    \n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Artificial Neural Networks': ann_classifier\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X, y, cv=k_fold, scoring=recall_precision_scorer)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Baseline',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "results = BaselineRunAll(df, 'Class')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution after Undersampling Majority Class:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.56  50000\n",
      "1   0.44    223\n",
      "Class Distribution after Oversampling Minority Class using SMOTE:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  50.00  50000\n",
      "1  50.00  50000\n",
      "Logistic Regression Model Training Completed\n",
      "Random Forest Model Training Completed\n",
      "K-Nearest Neighbours Model Training Completed\n"
     ]
    }
   ],
   "source": [
    "# Performs Undersampling of Majority Class followed by Oversampling of Minority class using SMOTE and tests on all 5 Algorithms\n",
    "def RandomSamplingSMOTE(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    # Undersample Majority Class\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: 50000, 1: 223}, random_state=42)\n",
    "    X, y = rus.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Undersampling Majority Class:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Oversample using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Oversampling Minority Class using SMOTE:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression()\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC()\n",
    "    ann_classifier = MLPClassifier(max_iter=1000)\n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), \n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "\n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Artificial Neural Networks': ann_classifier\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X, y, cv=k_fold, scoring=recall_precision_scorer)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Undersampling + SMOTE',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "results = RandomSamplingSMOTE(df, 'Class')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution after Undersampling Majority Class:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.56  50000\n",
      "1   0.44    223\n",
      "Class Distribution after Oversampling Minority Class using SMOTE:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  50.01  50000\n",
      "1  49.99  49985\n",
      "LR CV Completed\n",
      "RF CV Completed\n",
      "KNN CV Completed\n",
      "SVM CV Completed\n",
      "ANN CV Completed\n",
      "                 Method  Logistic Regression  Random Forest  \\\n",
      "0  SMOTE Class 1 Recall               0.9141         1.0000   \n",
      "\n",
      "   K-Nearest Neighbours  Support Vector Machines  Artificial Neural Networks  \n",
      "0                1.0000                   0.9997                      1.0000  \n"
     ]
    }
   ],
   "source": [
    "# Performs Undersampling of Majority Class followed by Oversampling of Minority class using ADASYN and tests on all 5 Algorithms\n",
    "def RandomSamplingADASYN(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    # Undersample Majority Class\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: 50000, 1: 223}, random_state=42)\n",
    "    X, y = rus.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Undersampling Majority Class:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Oversample using ADASYN\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X, y = adasyn.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Oversampling Minority Class using ADASYN:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression()\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC()\n",
    "    ann_classifier = MLPClassifier(max_iter=1000)\n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), \n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "\n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Artificial Neural Networks': ann_classifier\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X, y, cv=k_fold, scoring=recall_precision_scorer)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Undersampling + ADASYN',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "results = RandomSamplingADASYN(df, 'Class')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Model vs Recall graph for Classification Dataset for Each Method\n",
    "def plot_model_recall_graph(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotting lines for each technique\n",
    "    sns.lineplot(data=df, x='Classifier', y='Class 1 Recall', hue='Method', marker='o')\n",
    "\n",
    "    plt.title('Classifier vs Recall')\n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(title='Method')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Number of Folds vs R2 Score graph for Regression Dataset\n",
    "def plot_model_r2_graph(df):\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    \n",
    "    # Plotting line for Logistic Regression Accuracy\n",
    "    sns.lineplot(x=df['k'], y=df['Linear Regression R2 Score'], label='Linear Regression R2 Score', marker='o')\n",
    "\n",
    "    # Plotting line for Random Forest Accuracy\n",
    "    sns.lineplot(x=df['k'], y=df['Random Forest R2 Score'], label='Random Forest R2 Score', marker='o')\n",
    "\n",
    "    plt.title('Model R2 Score by Number of Folds')\n",
    "    plt.xlabel('Number of Folds (k)')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Number of Folds vs MSE Score graph for Regression Dataset\n",
    "def plot_model_mse_graph(df):\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    \n",
    "    # Plotting line for Logistic Regression Accuracy\n",
    "    sns.lineplot(x=df['k'], y=df['Linear Regression MSE Score'], label='Linear Regression MSE Score', marker='o')\n",
    "\n",
    "    # Plotting line for Random Forest Accuracy\n",
    "    sns.lineplot(x=df['k'], y=df['Random Forest MSE Score'], label='Random Forest MSE Score', marker='o')\n",
    "\n",
    "    plt.title('Model MSE Score by Number of Folds')\n",
    "    plt.xlabel('Number of Folds (k)')\n",
    "    plt.ylabel('MSE Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all Datasets into the required variables\n",
    "c_cancer, c_mice_expression, c_adult_income, r_life_expectancy, r_appartment_rent, r_song_popularity = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1 : Cancer Detection Dataset (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cancer\n",
    "# Checking for Null Values\n",
    "null_check(c_cancer)\n",
    "# No Null Values present hence Encoding Categorical Data to Numerical\n",
    "c_cancer = transform_data(c_cancer)\n",
    "# The target column in 'diagnosis' hence applying Logistic Regression with and without CV.\n",
    "c_cancer_results = LogisticRegressionCV(c_cancer, 'diagnosis', [0,5,10,20,50,100])\n",
    "# Displaying the graph of No. of Folds vs Accuracy\n",
    "plot_model_accuracy_graph(c_cancer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "As can be interpreted from the graph above, using CV yielded better results for Logistic Regression, but did not make much of an impact for Random Forest as the Algorithm is far more complex.\n",
    "However, a point of concern is that Logistic Regression has a better accuracy than Random Forest which means that the model is overfitting on Logistic Regression, or another valid explanation can be that the problem was better mapped by Logistic Regression than it was for Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2 : Mice Protein Expression Dataset (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mice_expression\n",
    "# Checking for Null Values\n",
    "null_check(c_mice_expression)\n",
    "# Null Values present hence Removing the data\n",
    "clean_data(c_mice_expression,[])\n",
    "# Encoding Categorical Data to Numerical\n",
    "c_mice_expression = transform_data(c_mice_expression)\n",
    "# The target column in 'class' hence applying Logistic Regression with and without CV.\n",
    "c_mice_expression_results = LogisticRegressionCV(c_mice_expression, 'class', [0,5,10,20,50,75])\n",
    "# Displaying the graph of No. of Folds vs Accuracy\n",
    "plot_model_accuracy_graph(c_mice_expression_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "The graph above shows the impact of using CV on the dataset. For Logistic Regression, the accuracy was better without CV, however, the accuracy decreased but then improved as the number of folds increased. One possible explanation for this can be that the data has 7 target classes and approximately 500 rows after data cleaning is performed. Hence when CV is used, there is a small chunk of data available to train the model from, hence the reduced accuracy. However, as the number of folds increase, the data available for training increases and hence accuracy increases as well as there is more data to train the model on. For Random Forest, without using CV, the model was overfitting on Training data hence the reduced accuracy, however, as CV helps models avoid overfitting, the test accuracy started increasing as the number of folds increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3 : Adult Income Dataset (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_adult_income\n",
    "# Checking for Null Values\n",
    "null_check(c_adult_income)\n",
    "# Null Values present hence Removing the data\n",
    "clean_data(c_adult_income,[])\n",
    "# Encoding Categorical Data to Numerical\n",
    "c_adult_income = transform_data(c_adult_income)\n",
    "# The target column in 'class' hence applying Logistic Regression with and without CV.\n",
    "c_adult_income_results = LogisticRegressionCV(c_adult_income, 'income', [0,5,10,20,50,100])\n",
    "# Displaying the graph of No. of Folds vs Accuracy\n",
    "plot_model_accuracy_graph(c_adult_income_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "The graph above shows the impact of using CV on the dataset. For Logistic Regression, the accuracy was some what constant as the data size was pretty large, hence even without cross validation, the model did not overfit. For Random Forest, the initial rise in accuracy and then the decrease can be explained by the fact that initially, the algorithm did overfit, but when CV was used, the generaliseability of the model improved and hence Accuracy decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 4 : Life Expectancy Dataset (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_life_expectancy\n",
    "# Checking for Null Values\n",
    "null_check(r_life_expectancy)\n",
    "# Null Values present hence Removing the data\n",
    "clean_data(r_life_expectancy,[])\n",
    "# Encoding Categorical Data to Numerical\n",
    "r_life_expectancy = transform_data(r_life_expectancy)\n",
    "# The target column in 'class' hence applying Logistic Regression with and without CV.\n",
    "r_life_expectancy_results = LinearRegressionCV(r_life_expectancy, 'Life expectancy ', [0,5,10,20,50,100])\n",
    "# Displaying the graph of No. of Folds vs R2\n",
    "plot_model_r2_graph(r_life_expectancy_results)\n",
    "# Displaying the graph of No. of Folds vs MSE\n",
    "plot_model_mse_graph(r_life_expectancy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "The graphs above shows the impact of using CV on the dataset. For Linear Regression, the R2 Score increased and then started decreasing, a possible cause of this could be limited size of the dataset. MSE score on the other hand, remained decreased minutely, and then remained constant. For Random Forest, the R2 Score increased, but then started decreasing, validating that the size of the dataset is small for large number of k-Folds. MSE for Random Forest decreased but then became constant.\n",
    "For reference, R2 score should be closer to 1 and MSE should be close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 5 : Appartment Rent Estimation (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_appartment_rent\n",
    "# Checking for Null Values\n",
    "null_check(r_appartment_rent)\n",
    "# No Null Values present hence Encoding Categorical Data to Numerical\n",
    "r_appartment_rent = transform_data(r_appartment_rent)\n",
    "# The target column in 'class' hence applying Logistic Regression with and without CV.\n",
    "r_appartment_rent_results = LinearRegressionCV(r_appartment_rent, 'price', [0,5,10,20,50,100])\n",
    "# Displaying the graph of No. of Folds vs R2\n",
    "plot_model_r2_graph(r_appartment_rent_results)\n",
    "# Displaying the graph of No. of Folds vs MSE\n",
    "plot_model_mse_graph(r_appartment_rent_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "The graphs above show the impact of using CV on the dataset. For Linear Regression, the R2 Score decreased drastically and is pretty low, illustrating that the model/algorithm is not a good fit to model the problem. Moreover, the drastic rise in MSE scores shows supports the above hypothesis as well as indicates that CV does avoid overfitting. For Random Forest, the R2 Score increased, and the MSE Score also rose drastically, showing that the model did overfit without CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 6 : Song Popularity Estimation (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_song_popularity\n",
    "# Checking for Null Values\n",
    "null_check(r_song_popularity)\n",
    "# No Null Values present hence Encoding Categorical Data to Numerical\n",
    "r_song_popularity = transform_data(r_song_popularity)\n",
    "# The target column in 'class' hence applying Logistic Regression with and without CV.\n",
    "r_song_popularity_results = LinearRegressionCV(r_song_popularity, 'song_popularity', [0,5,10,20,50,100])\n",
    "# Displaying the graph of No. of Folds vs R2\n",
    "plot_model_r2_graph(r_song_popularity_results)\n",
    "# Displaying the graph of No. of Folds vs MSE\n",
    "plot_model_mse_graph(r_song_popularity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "The graphs above show the impact of using CV on the dataset. For Linear Regression, the R2 Score and the MSE Score did not vary much with a low R2 Score, illustrating that the model/algorithm is not a good fit to model the problem. For Random Forest, the R2 Score increased, and the MSE Score also decreased when CV was used, showing that the model did better fit when CV was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "To Summarize the above results, the following things about CV can be concluded:\n",
    "1. CV has its many benefits of use, such as avoiding overfitting, improving model fit.\n",
    "2. In the above interpretations, CV mostly attributed to increase model fitting and sometimes even higher accuracies of model.\n",
    "3. A case where CV did perform poorly was where the size of dataset was small.\n",
    "4. Some general observations regarding performance are that the values of folds should not be too high for CV as it is time and compute intensive. Also increasing the folds not always helps the model fit better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
