{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Semester Project\n",
    "## Murtaza Hussain (29449) and Muhammad Asad ur Rehman (29456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Problem\n",
    "\n",
    "The below code solves the prevalent problem of imbalanced dataset, where one class dominates the dataset as compared to the other. Such is the case for the following dataset for Credit Card Transactions to detect Fraudulent Transactions. We will evaluate the following methods to resolve Class Imbalance:\n",
    "1. Random Under Sampling\n",
    "2. Algorithmic Methods (Using Random Forest as well as modifying Class Weights)\n",
    "3. Anomaly Detection Method\n",
    "\n",
    "For the following Dataset, we will use the following 5 Algorithms to draw a comparision between different methods:\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN)\n",
    "3. Random Forest\n",
    "4. Support Vector Machines (SVM)\n",
    "5. Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader loads data from CSV Files\n",
    "def load_dataset():\n",
    "    dataset = pd.read_csv(\"./Source.CreditCardFraud.csv\")\n",
    "    return dataset\n",
    "\n",
    "# df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: Time\n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V1\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V2\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V3\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V4\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V5\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V6\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V7\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V8\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V9\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V10\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V11\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V12\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V13\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V14\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V15\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V16\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V17\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V18\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V19\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V20\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V21\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V22\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V23\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V24\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V25\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V26\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V27\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: V28\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Amount\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Class\n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function performs a missing value analysis on each column of the dataset, helps you decide on what to do in cleaning process\n",
    "def null_check(df):\n",
    "    null_columns = []\n",
    "    for column in df.columns:\n",
    "        print(\"Column Name:\", column)\n",
    "        print(\"Column DataType:\", df[column].dtype)\n",
    "        if df[column].dtype != 'float64' and df[column].dtype != 'int64':\n",
    "            print(\"Column unique values:\", df[column].unique())\n",
    "        print(\"Column has null:\", df[column].isnull().any())\n",
    "\n",
    "        \n",
    "        if df[column].isnull().any() == True:\n",
    "            print(\"Column Null Count:\", df[column].isnull().sum())\n",
    "            null_columns.append(column)\n",
    "        print(\"\\n\")\n",
    "    return null_columns\n",
    "\n",
    "# null_check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops any null columns and missing values\n",
    "# This is where you decide whether to remove NULL rows (which will reduce the size of Dataset) or remove NULL columns entirely. You can also choose a combination of both.\n",
    "def clean_data(df, drop_columns, missing_value = False):\n",
    "    # Remove unnecessary columns\n",
    "    df.drop(drop_columns, axis=1, inplace=True)\n",
    "    # Drop rows with any missing values\n",
    "    if missing_value == False:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        df.fillna(missing_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.78  99776\n",
      "1   0.22    223\n"
     ]
    }
   ],
   "source": [
    "# Prints a summary of class instances and distribution\n",
    "def data_summary(df, target=None):\n",
    "    if isinstance(df, pd.DataFrame) and target!=None:\n",
    "        a = df[target].value_counts()\n",
    "    else:\n",
    "        a = df.value_counts()\n",
    "    class0 = format(100 * a[0]/sum(a), \".2f\")\n",
    "    class1 = format(100 * a[1]/sum(a), \".2f\")\n",
    "\n",
    "    meta = pd.DataFrame([{ \"%\": class0, \"count\": a[0]},\n",
    "                         { \"%\": class1, \"count\": a[1]}])\n",
    "    print(\"\\nClass Distribution:\\n\", meta, \"\\n\")\n",
    "\n",
    "# data_summary(df,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: Index(['Time', 'Class'], dtype='object')\n",
      "Numerical columns: Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
      "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
      "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Transforms categorical and numberical data into numerical data\n",
    "def transform_data(df):\n",
    "    # Encode categorical variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    print(\"Categorical columns:\", df.select_dtypes(include=['object', 'int64']).columns)\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Numerical columns:\", df.select_dtypes(include=['float64']).columns)\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    return df\n",
    "\n",
    "# df['Class'] = df['Class'].astype(str)\n",
    "# df = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution for Baseline Run:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.78  99776\n",
      "1   0.22    223\n",
      "Logistic Regression Model Training Completed\n",
      "Random Forest Model Training Completed\n",
      "K-Nearest Neighbours Model Training Completed\n",
      "Support Vector Machines Model Training Completed\n",
      "Artificial Neural Networks Model Training Completed\n",
      "     Method                  Classifier  Class 1 Recall  Class 1 Precision\n",
      "0  Baseline         Logistic Regression          0.5648             0.7965\n",
      "1  Baseline               Random Forest          0.8383             0.9645\n",
      "2  Baseline        K-Nearest Neighbours          0.8298             0.9415\n",
      "3  Baseline     Support Vector Machines          0.7265             0.9697\n",
      "4  Baseline  Artificial Neural Networks          0.8385             0.9559\n"
     ]
    }
   ],
   "source": [
    "# Runs Baseline Model for All 5 Algorithms\n",
    "def BaselineRunAll(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    print(\"Class Distribution for Baseline Run:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression()\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC()\n",
    "    ann_classifier = MLPClassifier(max_iter=1000)\n",
    "    \n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)     # The reasoning behind k = 10 is so as to strike a balance between test and train samples of minority class\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), # As the majority class has 99.81% presence, accuracy cannot be used as a metric to evaluate performance\n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "    \n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Artificial Neural Networks': ann_classifier\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X, y, cv=k_fold, scoring=recall_precision_scorer)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Baseline',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "results = BaselineRunAll(df, 'Class')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution after Undersampling Majority Class:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.56  50000\n",
      "1   0.44    223\n",
      "Class Distribution after Oversampling Minority Class using SMOTE:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  50.00  50000\n",
      "1  50.00  50000\n",
      "Logistic Regression Model Training Completed\n",
      "Random Forest Model Training Completed\n",
      "K-Nearest Neighbours Model Training Completed\n",
      "Support Vector Machines Model Training Completed\n"
     ]
    }
   ],
   "source": [
    "# Performs Undersampling of Majority Class followed by Oversampling of Minority class using SMOTE and tests on all 5 Algorithms\n",
    "def RandomSamplingSMOTE(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    # Undersample Majority Class\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: 50000, 1: 223}, random_state=42)\n",
    "    X, y = rus.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Undersampling Majority Class:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Oversample using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Oversampling Minority Class using SMOTE:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression()\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC()\n",
    "    ann_classifier = MLPClassifier(max_iter=1000)\n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), \n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "\n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Artificial Neural Networks': ann_classifier\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X, y, cv=k_fold, scoring=recall_precision_scorer)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Undersampling + SMOTE',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "results = RandomSamplingSMOTE(df, 'Class')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution after Undersampling Majority Class:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  99.56  50000\n",
      "1   0.44    223\n",
      "Class Distribution after Oversampling Minority Class using SMOTE:\n",
      "Class Distribution:\n",
      "\n",
      "        %  count\n",
      "0  50.01  50000\n",
      "1  49.99  49985\n",
      "LR CV Completed\n",
      "RF CV Completed\n",
      "KNN CV Completed\n",
      "SVM CV Completed\n",
      "ANN CV Completed\n",
      "                 Method  Logistic Regression  Random Forest  \\\n",
      "0  SMOTE Class 1 Recall               0.9141         1.0000   \n",
      "\n",
      "   K-Nearest Neighbours  Support Vector Machines  Artificial Neural Networks  \n",
      "0                1.0000                   0.9997                      1.0000  \n"
     ]
    }
   ],
   "source": [
    "# Performs Undersampling of Majority Class followed by Oversampling of Minority class using ADASYN and tests on all 5 Algorithms\n",
    "def RandomSamplingADASYN(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    # Undersample Majority Class\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: 50000, 1: 223}, random_state=42)\n",
    "    X, y = rus.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Undersampling Majority Class:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Oversample using ADASYN\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X, y = adasyn.fit_resample(X, y)\n",
    "    print(\"Class Distribution after Oversampling Minority Class using ADASYN:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression()\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC()\n",
    "    ann_classifier = MLPClassifier(max_iter=1000)\n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), \n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "\n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Artificial Neural Networks': ann_classifier\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X, y, cv=k_fold, scoring=recall_precision_scorer)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Undersampling + ADASYN',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "results = RandomSamplingADASYN(df, 'Class')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Model vs Recall graph for Classification Dataset for Each Method\n",
    "def plot_model_recall_graph(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotting lines for each Method\n",
    "    sns.lineplot(data=df, x='Classifier', y='Class 1 Recall', hue='Method', marker='o')\n",
    "\n",
    "    plt.title('Classifier vs Recall')\n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(title='Method')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Model vs Precision graph for Classification Dataset for Each Method\n",
    "def plot_model_precision_graph(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotting lines for each Method\n",
    "    sns.lineplot(data=df, x='Classifier', y='Class 1 Precision', hue='Method', marker='o')\n",
    "\n",
    "    plt.title('Classifier vs Precision')\n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(title='Method')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_workflow():\n",
    "    # Load Dataset\n",
    "    df = load_dataset()\n",
    "    # No need for Data Cleaning and EDA as Data is already clean\n",
    "    # Evaluate Class Distribution of the cleaned Dataset\n",
    "    data_summary(df)\n",
    "    # Transform and Encode Data\n",
    "    df['Class'] = df['Class'].astype(str)\n",
    "    df = transform_data(df)\n",
    "    # Run Baseline Models using k = 10\n",
    "    baseline_results = BaselineRunAll(df, 'Class')\n",
    "    # Evaluate Models using SMOTE Oversampling Technique\n",
    "    smote_results = RandomSamplingSMOTE(df, 'Class')\n",
    "    # Evaluate Models using ADASYN Oversampling Technique\n",
    "    adasyn_results = RandomSamplingADASYN(df, 'Class')\n",
    "    # Concatenate the results\n",
    "    results_df = pd.concat(baseline_results, smote_results, adasyn_results)\n",
    "    # Plot a Classifier vs Recall Graph -> To evaluate how well the model is performing to detect the fraudulent transactions (minority class)\n",
    "    plot_model_recall_graph(results_df)\n",
    "    # Plot a Classifier vs Precision Graph -> To evaluate how precise the model is to detect the minority class (can be used as a secondary metric for evaluation)\n",
    "    plot_model_precision_graph(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
