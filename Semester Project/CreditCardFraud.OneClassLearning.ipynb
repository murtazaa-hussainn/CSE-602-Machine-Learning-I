{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Semester Project\n",
    "## Murtaza Hussain (29449) and Muhammad Asad ur Rehman (29456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Problem\n",
    "\n",
    "The below code solves the prevalent problem of imbalanced dataset, where one class dominates the dataset as compared to the other. Such is the case for the following dataset for Credit Card Transactions to detect Fraudulent Transactions. We will evaluate the following methods to resolve Class Imbalance:\n",
    "1. Random Under Sampling\n",
    "2. Algorithmic Methods (Using Random Forest as well as modifying Class Weights)\n",
    "3. Anomaly Detection Method\n",
    "\n",
    "For the following Dataset, we will use the following 5 Algorithms to draw a comparision between different methods:\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN)\n",
    "3. Random Forest\n",
    "4. Support Vector Machines (SVM)\n",
    "5. Naive Bayes (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score\n",
    "from sklearn.model_selection import cross_validate, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader loads data from CSV Files\n",
    "def load_dataset():\n",
    "    dataset = pd.read_csv(\"./Source.CreditCardFraud.csv\")\n",
    "    return dataset\n",
    "\n",
    "# df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a missing value analysis on each column of the dataset, helps you decide on what to do in cleaning process\n",
    "def null_check(df):\n",
    "    null_columns = []\n",
    "    for column in df.columns:\n",
    "        print(\"Column Name:\", column)\n",
    "        print(\"Column DataType:\", df[column].dtype)\n",
    "        if df[column].dtype != 'float64' and df[column].dtype != 'int64':\n",
    "            print(\"Column unique values:\", df[column].unique())\n",
    "        print(\"Column has null:\", df[column].isnull().any())\n",
    "\n",
    "        \n",
    "        if df[column].isnull().any() == True:\n",
    "            print(\"Column Null Count:\", df[column].isnull().sum())\n",
    "            null_columns.append(column)\n",
    "        print(\"\\n\")\n",
    "    return null_columns\n",
    "\n",
    "# null_check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops any null columns and missing values\n",
    "# This is where you decide whether to remove NULL rows (which will reduce the size of Dataset) or remove NULL columns entirely. You can also choose a combination of both.\n",
    "def clean_data(df, drop_columns, missing_value = False):\n",
    "    # Remove unnecessary columns\n",
    "    df.drop(drop_columns, axis=1, inplace=True)\n",
    "    # Drop rows with any missing values\n",
    "    if missing_value == False:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        df.fillna(missing_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a summary of class instances and distribution\n",
    "def data_summary(df, target=None):\n",
    "    if isinstance(df, pd.DataFrame) and target!=None:\n",
    "        a = df[target].value_counts()\n",
    "    else:\n",
    "        a = df.value_counts()\n",
    "    class0 = format(100 * a[0]/sum(a), \".2f\")\n",
    "    class1 = format(100 * a[1]/sum(a), \".2f\")\n",
    "\n",
    "    meta = pd.DataFrame([{ \"%\": class0, \"count\": a[0]},\n",
    "                         { \"%\": class1, \"count\": a[1]}])\n",
    "    print(\"\\nClass Distribution:\\n\", meta, \"\\n\")\n",
    "\n",
    "# data_summary(df,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms categorical and numberical data into numerical data\n",
    "def transform_data(df):\n",
    "    # Encode categorical variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    print(\"Categorical columns:\", df.select_dtypes(include=['object', 'int64']).columns)\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Numerical columns:\", df.select_dtypes(include=['float64']).columns)\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    return df\n",
    "\n",
    "# df['Class'] = df['Class'].astype(str)\n",
    "# df = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs Baseline Model for All 5 Algorithms\n",
    "def BaselineRunAll(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    print(\"Class Distribution for Baseline Run:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression(max_iter=1000)\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC(probability=True)\n",
    "    nb_classifier = GaussianNB()\n",
    "    \n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)     # The reasoning behind k = 10 is so as to strike a balance between test and train samples of minority class\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), # As the majority class has 99.81% presence, accuracy cannot be used as a metric to evaluate performance\n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "    \n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Naive Bayes': nb_classifier\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X, y, cv=k_fold, scoring=recall_precision_scorer)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Baseline',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# results = BaselineRunAll(df, 'Class')\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs One Class Learning (Majority Class) on all 5 Algorithms\n",
    "def OneClassLearning(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    print(\"Class Distribution for One Class Learning Majority Class:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "    lr_classifier = LogisticRegression(max_iter=1000)\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    svm_classifier = SVC(probability=True)\n",
    "    nb_classifier = GaussianNB()\n",
    "\n",
    "    # Separate minority and majority class data\n",
    "    X_minority = X[y == 1]\n",
    "    X_majority = X[y == 0]\n",
    "    y_minority = y[y == 1]\n",
    "    y_majority = y[y == 0]\n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10 (Will have to change the cross validation technique)\n",
    "    k_fold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    majority_indices = k_fold.split(X_majority)\n",
    "\n",
    "    # Using last fold as the test fold which includes all minority data\n",
    "    train_indices = [train_index for train_index, _ in majority_indices]\n",
    "    test_indices = train_indices.pop()  # We use the last fold's train_index as our test set index\n",
    "\n",
    "    X_train = X_majority.iloc[np.concatenate(train_indices)]\n",
    "    X_test = pd.concat([X_majority.iloc[test_indices], X_minority])\n",
    "    y_train = y_majority.iloc[np.concatenate(train_indices)]\n",
    "    y_test = pd.concat([y_majority.iloc[test_indices], y_minority])\n",
    "\n",
    "    classifiers = {\n",
    "        'Logistic Regression': lr_classifier,\n",
    "        'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        'Naive Bayes': nb_classifier\n",
    "    }\n",
    "\n",
    "    # Train on majority class only\n",
    "    majority_class = (y == 0)\n",
    "    X_majority = X[majority_class]\n",
    "    y_majority = y[majority_class]\n",
    "\n",
    "    # Manual Training and evaluating classifiers\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        y_pred = clf.predict(X_test)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "        precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Undersampling + SMOTE',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': recall,\n",
    "            'Class 1 Precision': precision\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# results = RandomSamplingSMOTE(df, 'Class')\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs One Class Learning (Majority Class) using special methods\n",
    "def OneClassLearningSpecialMethods(df, target_name, k=10):\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = df.drop(target_name, axis=1)\n",
    "    y = df[target_name]\n",
    "    results = []\n",
    "\n",
    "    print(\"Class Distribution for One Class Learning Special Methods:\")\n",
    "    data_summary(y)\n",
    "\n",
    "    # Initialize the classifiers\n",
    "#    lr_classifier = LogisticRegression(max_iter=1000) # Has no such implementation\n",
    "#    rf_classifier = RandomForestClassifier() # Has no such implementation\n",
    "    knn_classifier = LocalOutlierFactor(novelty=True) # A special implementation of KNN for One Class Learning\n",
    "    svm_classifier = OneClassSVM(kernel='rbf', gamma='auto') # A special implementation of SVM for One Class Learning\n",
    "#    nb_classifier = GaussianNB() # Has no such implementation\n",
    "\n",
    "    # Initialize k-fold cross-validation where folds = 10\n",
    "    k_fold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a recall and precision scorer specifically focusing on the minority class\n",
    "    recall_precision_scorer = {'recall': make_scorer(recall_score, pos_label=1), \n",
    "                               'precision': make_scorer(precision_score, pos_label=1)}\n",
    "\n",
    "    classifiers = {\n",
    "        # 'Logistic Regression': lr_classifier,\n",
    "        # 'Random Forest': rf_classifier,\n",
    "        'K-Nearest Neighbours': knn_classifier,\n",
    "        'Support Vector Machines': svm_classifier,\n",
    "        # 'Naive Bayes': nb_classifier\n",
    "    }\n",
    "\n",
    "    # Train on majority class only\n",
    "    majority_class = (y == 0)\n",
    "    X_majority = X[majority_class]\n",
    "    y_majority = y[majority_class]\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        scores = cross_validate(clf, X_majority, y_majority, cv=k_fold, scoring=recall_precision_scorer, return_train_score=False)\n",
    "        print(f\"{clf_name} Model Training Completed\")\n",
    "        mean_recall = scores['test_recall'].mean()\n",
    "        mean_precision = scores['test_precision'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Undersampling + SMOTE',\n",
    "            'Classifier': clf_name,\n",
    "            'Class 1 Recall': mean_recall,\n",
    "            'Class 1 Precision': mean_precision\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# results = RandomSamplingSMOTE(df, 'Class')\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Model vs Recall graph for Classification Dataset for Each Method\n",
    "def plot_model_recall_graph(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotting lines for each Method\n",
    "    sns.lineplot(data=df, x='Classifier', y='Class 1 Recall', hue='Method', marker='o')\n",
    "\n",
    "    plt.title('Classifier vs Recall')\n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(title='Method')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Model vs Precision graph for Classification Dataset for Each Method\n",
    "def plot_model_precision_graph(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotting lines for each Method\n",
    "    sns.lineplot(data=df, x='Classifier', y='Class 1 Precision', hue='Method', marker='o')\n",
    "\n",
    "    plt.title('Classifier vs Precision')\n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(title='Method')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_workflow():\n",
    "    # Load Dataset\n",
    "    df = load_dataset()\n",
    "    # No need for Data Cleaning and EDA as Data is already clean\n",
    "    # Evaluate Class Distribution of the cleaned Dataset\n",
    "    data_summary(df,'Class')\n",
    "    # Transform and Encode Data\n",
    "    df['Class'] = df['Class'].astype(str)\n",
    "    df = transform_data(df)\n",
    "    # Run Baseline Models using k = 10\n",
    "    baseline_results = BaselineRunAll(df, 'Class')\n",
    "    # Evaluate Models using SMOTE Oversampling Technique\n",
    "    smote_results = RandomSamplingSMOTE(df, 'Class')\n",
    "    # Evaluate Models using ADASYN Oversampling Technique\n",
    "    adasyn_results = RandomSamplingADASYN(df, 'Class')\n",
    "    # Concatenate the results\n",
    "    results_df = pd.concat([baseline_results, smote_results, adasyn_results])\n",
    "    # Print results\n",
    "    print(results_df)\n",
    "    results_df.to_csv('SamplingMethodsResults.csv', index=False)\n",
    "    # Plot a Classifier vs Recall Graph -> To evaluate how well the model is performing to detect the fraudulent transactions (minority class)\n",
    "    plot_model_recall_graph(results_df)\n",
    "    # Plot a Classifier vs Precision Graph -> To evaluate how precise the model is to detect the minority class (can be used as a secondary metric for evaluation)\n",
    "    plot_model_precision_graph(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the pipeline was not running in one go, we had to split it into smaller parts\n",
    "# master_workflow() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broken down pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = load_dataset()\n",
    "# No need for Data Cleaning and EDA as Data is already clean\n",
    "# Evaluate Class Distribution of the cleaned Dataset\n",
    "data_summary(df,'Class')\n",
    "# Transform and Encode Data\n",
    "df['Class'] = df['Class'].astype(str)\n",
    "df = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Baseline Models using k = 10\n",
    "baseline_results = BaselineRunAll(df, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Models using SMOTE Oversampling Technique\n",
    "one_class_results = OneClassLearning(df, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Models using ADASYN Oversampling Technique\n",
    "adasyn_results = RandomSamplingADASYN(df, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the results\n",
    "results_df = pd.concat([baseline_results, smote_results, adasyn_results])\n",
    "# Print results\n",
    "print(results_df)\n",
    "results_df.to_csv('SamplingMethodsResults.csv', index=False)\n",
    "# Plot a Classifier vs Recall Graph -> To evaluate how well the model is performing to detect the fraudulent transactions (minority class)\n",
    "plot_model_recall_graph(results_df)\n",
    "# Plot a Classifier vs Precision Graph -> To evaluate how precise the model is to detect the minority class (can be used as a secondary metric for evaluation)\n",
    "plot_model_precision_graph(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
