{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4a549e",
   "metadata": {},
   "source": [
    "# Machine Learning Excel Assignment \n",
    "## Murtaza Hussain (29449) and Muhammad Asad ur Rehman (29456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26133ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report, r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, chi2, SequentialFeatureSelector\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lazypredict.Supervised import LazyClassifier, LazyRegressor\n",
    "from IPython.display import display\n",
    "import statsmodels.api as sm\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba375ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader loads data from UCI-ML Repo\n",
    "def load_datasets():\n",
    "    # Classification Datasets\n",
    "    c_cancer = pd.read_csv(\"./Classification.CancerMB.csv\").iloc[:, :32]\n",
    "    # Regression Datasets\n",
    "    r_life_expectancy = pd.read_csv(\"./Regression.Life.Expectancy.csv\")\n",
    "    \n",
    "    return c_cancer, r_life_expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa58952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a missing value analysis on each column of the dataset, helps you decide on what to do in cleaning process\n",
    "def null_check(df):\n",
    "    null_columns = []\n",
    "    for column in df.columns:\n",
    "        print(\"Column Name:\", column)\n",
    "        print(\"Column DataType:\", df[column].dtype)\n",
    "        if df[column].dtype != 'float64' and df[column].dtype != 'int64':\n",
    "            print(\"Column unique values:\", df[column].unique())\n",
    "        print(\"Column has null:\", df[column].isnull().any())\n",
    "\n",
    "        \n",
    "        if df[column].isnull().any() == True:\n",
    "            print(\"Column Null Count:\", df[column].isnull().sum())\n",
    "            null_columns.append(column)\n",
    "        print(\"\\n\")\n",
    "    return null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b47f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops any null columns and missing values\n",
    "# This is where you decide whether to remove NULL rows (which will reduce the size of Dataset) or remove NULL columns entirely. You can also choose a combination of both.\n",
    "def clean_data(df, drop_columns, missing_value = False):\n",
    "    # Remove unnecessary columns\n",
    "    df.drop(drop_columns, axis=1, inplace=True)\n",
    "    # Drop rows with any missing values\n",
    "    if missing_value == False:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        df.fillna(missing_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e773146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms categorical and numberical data into numerical data\n",
    "def transform_data(df):\n",
    "    # Encode categorical variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    # print(\"Categorical columns:\", df.select_dtypes(include=['object']).columns)\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    # print(\"Numerical columns:\", df.select_dtypes(include=['float64', 'int64']).columns)\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0fad7",
   "metadata": {},
   "source": [
    "# Classificiation Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fc5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNScore(data, target_col, cv_splits):\n",
    "    # Split dataset into features and target variable\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Initialize KFold with different values\n",
    "    results = {}\n",
    "\n",
    "    for cv in cv_splits:\n",
    "        k_fold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store scores\n",
    "        knn_f1_positive_scores = []\n",
    "        knn_f1_negative_scores = []\n",
    "        knn_auc_scores = []\n",
    "        knn_accuracy_scores = []\n",
    "\n",
    "        for train_index, test_index in k_fold.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # Standardize features by removing the mean and scaling to unit variance\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # KNN Classifier\n",
    "            knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "            knn_classifier.fit(X_train_scaled, y_train)\n",
    "            knn_y_pred = knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "            # Calculate evaluation metrics for KNN\n",
    "            knn_f1_positive_scores.append(f1_score(y_test, knn_y_pred, pos_label=1))\n",
    "            knn_f1_negative_scores.append(f1_score(y_test, knn_y_pred, pos_label=0))\n",
    "            knn_auc_scores.append(roc_auc_score(y_test, knn_y_pred))\n",
    "            knn_accuracy_scores.append(accuracy_score(y_test, knn_y_pred))\n",
    "\n",
    "        # Calculate average scores\n",
    "        avg_f1_positive_score = sum(knn_f1_positive_scores) / len(knn_f1_positive_scores)\n",
    "        avg_f1_negative_score = sum(knn_f1_negative_scores) / len(knn_f1_negative_scores)\n",
    "        avg_auc_score = sum(knn_auc_scores) / len(knn_auc_scores)\n",
    "        avg_accuracy = sum(knn_accuracy_scores) / len(knn_accuracy_scores)\n",
    "\n",
    "        # Store results\n",
    "        results[f'CV_{cv}'] = {\n",
    "            'F1 Score (Positive)': avg_f1_positive_score,\n",
    "            'F1 Score (Negative)': avg_f1_negative_score,\n",
    "            'AUC Score': avg_auc_score,\n",
    "            'Accuracy': avg_accuracy\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index', columns=['F1 Score (Positive)', 'F1 Score (Negative)', 'AUC Score', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33583631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionScore(data, target_col, cv_splits):\n",
    "    # Split dataset into features and target variable\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Initialize dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    for cv in cv_splits:\n",
    "        # Initialize KFold\n",
    "        k_fold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store scores\n",
    "        log_reg_f1_positive_scores = []\n",
    "        log_reg_f1_negative_scores = []\n",
    "        log_reg_auc_scores = []\n",
    "        log_reg_accuracy_scores = []\n",
    "\n",
    "        for train_index, test_index in k_fold.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # Standardize features by removing the mean and scaling to unit variance\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Logistic Regression\n",
    "            log_reg_classifier = LogisticRegression()\n",
    "            log_reg_classifier.fit(X_train_scaled, y_train)\n",
    "            log_reg_y_pred = log_reg_classifier.predict(X_test_scaled)\n",
    "\n",
    "            # Calculate evaluation metrics for Logistic Regression\n",
    "            log_reg_f1_positive_scores.append(f1_score(y_test, log_reg_y_pred, pos_label=1))\n",
    "            log_reg_f1_negative_scores.append(f1_score(y_test, log_reg_y_pred, pos_label=0))\n",
    "            log_reg_auc_scores.append(roc_auc_score(y_test, log_reg_y_pred))\n",
    "            log_reg_accuracy_scores.append(accuracy_score(y_test, log_reg_y_pred))\n",
    "\n",
    "        # Calculate average scores\n",
    "        avg_f1_positive_score = sum(log_reg_f1_positive_scores) / len(log_reg_f1_positive_scores)\n",
    "        avg_f1_negative_score = sum(log_reg_f1_negative_scores) / len(log_reg_f1_negative_scores)\n",
    "        avg_auc_score = sum(log_reg_auc_scores) / len(log_reg_auc_scores)\n",
    "        avg_accuracy = sum(log_reg_accuracy_scores) / len(log_reg_accuracy_scores)\n",
    "\n",
    "        # Store results\n",
    "        results[f'CV_{cv}'] = {\n",
    "            'F1 Score (Positive)': avg_f1_positive_score,\n",
    "            'F1 Score (Negative)': avg_f1_negative_score,\n",
    "            'AUC Score': avg_auc_score,\n",
    "            'Accuracy': avg_accuracy\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index', columns=['F1 Score (Positive)', 'F1 Score (Negative)', 'AUC Score', 'Accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e75ec",
   "metadata": {},
   "source": [
    "# Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f7be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNRegressionMetrics(data, target_col, cv_splits):\n",
    "    # Split dataset into features and target variable\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Define the number of neighbors for KNN\n",
    "    n_neighbors = 5\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for cv in cv_splits:\n",
    "        # Initialize lists to store scores for each CV split\n",
    "        mae_scores = []\n",
    "        mse_scores = []\n",
    "        rmse_scores = []\n",
    "        r_squared_scores = []\n",
    "        aic_scores = []\n",
    "        bic_scores = []\n",
    "        p_values_list = []\n",
    "\n",
    "        # Initialize KFold\n",
    "        k_fold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_index, test_index in k_fold.split(X):\n",
    "            # Reset scores for each fold within a CV split\n",
    "            fold_mae = []\n",
    "            fold_mse = []\n",
    "            fold_rmse = []\n",
    "            fold_r_squared = []\n",
    "            fold_aic = []\n",
    "            fold_bic = []\n",
    "            fold_p_values = []\n",
    "\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # Standardize features by removing the mean and scaling to unit variance\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # KNN Regressor\n",
    "            knn_regressor = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "            knn_regressor.fit(X_train_scaled, y_train)\n",
    "            y_pred = knn_regressor.predict(X_test_scaled)\n",
    "\n",
    "            # Calculate evaluation metrics for this fold\n",
    "            fold_mae.append(mean_absolute_error(y_test, y_pred))\n",
    "            fold_mse.append(mean_squared_error(y_test, y_pred))\n",
    "            fold_rmse.append(mean_squared_error(y_test, y_pred, squared=False))\n",
    "            fold_r_squared.append(r2_score(y_test, y_pred))\n",
    "\n",
    "            # AIC, BIC, and p-values using OLS regression\n",
    "            X_train_sm = sm.add_constant(X_train)\n",
    "            ols_model = sm.OLS(y_train, X_train_sm)\n",
    "            ols_results = ols_model.fit()\n",
    "            fold_aic.append(ols_results.aic)\n",
    "            fold_bic.append(ols_results.bic)\n",
    "            fold_p_values.append(ols_results.pvalues.drop('const'))  # Exclude p-value of intercept\n",
    "\n",
    "            # Append scores for this fold\n",
    "            mae_scores.append(fold_mae)\n",
    "            mse_scores.append(fold_mse)\n",
    "            rmse_scores.append(fold_rmse)\n",
    "            r_squared_scores.append(fold_r_squared)\n",
    "            aic_scores.append(fold_aic)\n",
    "            bic_scores.append(fold_bic)\n",
    "            p_values_list.append(fold_p_values)\n",
    "\n",
    "        # Calculate average scores across all folds for this CV split\n",
    "        avg_mae = np.mean(mae_scores)\n",
    "        avg_mse = np.mean(mse_scores)\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        avg_r_squared = np.mean(r_squared_scores)\n",
    "        avg_aic = np.mean(aic_scores)\n",
    "        avg_bic = np.mean(bic_scores)\n",
    "        avg_p_values = np.mean(p_values_list, axis=0)  # Take the mean along the columns\n",
    "\n",
    "        # Flatten the avg_p_values array\n",
    "        flattened_p_values = [item for sublist in avg_p_values for item in sublist]\n",
    "        \n",
    "        # Calculate the average p-value\n",
    "        avg_p_value = np.mean(flattened_p_values)\n",
    "        \n",
    "        # Store results\n",
    "        results[f'CV_{cv}'] = {\n",
    "            'MAE': avg_mae,\n",
    "            'MSE': avg_mse,\n",
    "            'RMSE': avg_rmse,\n",
    "            'R-Squared': avg_r_squared,\n",
    "            'AIC': avg_aic,\n",
    "            'BIC': avg_bic,\n",
    "            'P-Values': avg_p_value\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83739128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLSRegressionMetrics(data, target_col, cv_splits):\n",
    "    # Split dataset into features and target variable\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for cv in cv_splits:\n",
    "        # Initialize KFold\n",
    "        k_fold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "        # Lists to store scores\n",
    "        mae_scores = []\n",
    "        mse_scores = []\n",
    "        rmse_scores = []\n",
    "        r_squared_scores = []\n",
    "        aic_scores = []\n",
    "        bic_scores = []\n",
    "        p_values_list = []\n",
    "\n",
    "        for train_index, test_index in k_fold.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # Standardize features by removing the mean and scaling to unit variance\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Add constant to features for OLS\n",
    "            X_train_sm = sm.add_constant(X_train_scaled)\n",
    "\n",
    "            # OLS Regression\n",
    "            ols_model = sm.OLS(y_train, X_train_sm)\n",
    "            ols_results = ols_model.fit()\n",
    "\n",
    "            # Predictions\n",
    "            X_test_sm = sm.add_constant(X_test_scaled)\n",
    "            y_pred = ols_results.predict(X_test_sm)\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            r_squared = r2_score(y_test, y_pred)\n",
    "            aic = ols_results.aic\n",
    "            bic = ols_results.bic\n",
    "            p_values = ols_results.pvalues.drop('const')  # Exclude p-value of intercept\n",
    "\n",
    "            # Append scores\n",
    "            mae_scores.append(mae)\n",
    "            mse_scores.append(mse)\n",
    "            rmse_scores.append(rmse)\n",
    "            r_squared_scores.append(r_squared)\n",
    "            aic_scores.append(aic)\n",
    "            bic_scores.append(bic)\n",
    "            p_values_list.append(p_values)\n",
    "\n",
    "        # Calculate the average scores for each metric\n",
    "        avg_mae = np.mean(mae_scores)\n",
    "        avg_mse = np.mean(mse_scores)\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        avg_r_squared = np.mean(r_squared_scores)\n",
    "        avg_aic = np.mean(aic_scores)\n",
    "        avg_bic = np.mean(bic_scores)\n",
    "\n",
    "        # Flatten the nested lists of p-values\n",
    "        flattened_p_values = [item for sublist in p_values_list for item in sublist]\n",
    "\n",
    "        # Calculate the average p-value for each feature\n",
    "        avg_p_values = np.mean(flattened_p_values, axis=0)\n",
    "\n",
    "        # Store results\n",
    "        results[f'CV_{cv}'] = {\n",
    "            'MAE': avg_mae,\n",
    "            'MSE': avg_mse,\n",
    "            'RMSE': avg_rmse,\n",
    "            'R-Squared': avg_r_squared,\n",
    "            'AIC': avg_aic,\n",
    "            'BIC': avg_bic,\n",
    "            'P-Values': avg_p_values.tolist()  # Convert numpy array to list\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f3224",
   "metadata": {},
   "source": [
    "# Lazy Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6580f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LazyPredictClassification(data, target_col, cv_splits):\n",
    "    # Extract features and target variable\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for cv in cv_splits:\n",
    "        # Split the data into training and testing sets for cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/cv, random_state=42)\n",
    "\n",
    "        # Initialize LazyClassifier\n",
    "        clf = LazyClassifier(predictions=True, verbose=0, ignore_warnings=True)\n",
    "\n",
    "        # Fit LazyClassifier\n",
    "        models, _ = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # Extract accuracy\n",
    "        accuracy = models['Accuracy']\n",
    "        \n",
    "        results[f'CV_{cv}'] = accuracy\n",
    "\n",
    "    return pd.concat(results.values(), keys=results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb556a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LazyPredictRegression(data, target_col, cv_splits):\n",
    "    # Extract features and target variable\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for cv in cv_splits:\n",
    "        # Split the data into training and testing sets for cross-validation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/cv, random_state=42)\n",
    "\n",
    "        # Initialize LazyRegressor\n",
    "        reg = LazyRegressor(predictions=True, verbose=0, ignore_warnings=True)\n",
    "\n",
    "        # Fit LazyRegressor\n",
    "        models, _ = reg.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # Extract RMSE and R-squared values\n",
    "        metrics = models[['RMSE', 'R-Squared']]\n",
    "        \n",
    "        results[f'CV_{cv}'] = metrics\n",
    "\n",
    "    return pd.concat(results.values(), keys=results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266e3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all Datasets into the required variables\n",
    "c_cancer, r_life_expectancy = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6834541",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ffb6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1c513",
   "metadata": {},
   "source": [
    "## Dataset 1: Cancer Detection Dataset (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "183608f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: id\n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: diagnosis\n",
      "Column DataType: object\n",
      "Column unique values: ['M' 'B']\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: radius_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: texture_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: perimeter_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: area_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: smoothness_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: compactness_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: concavity_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: concave points_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: symmetry_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: fractal_dimension_mean\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: radius_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: texture_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: perimeter_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: area_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: smoothness_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: compactness_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: concavity_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: concave points_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: symmetry_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: fractal_dimension_se\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: radius_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: texture_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: perimeter_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: area_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: smoothness_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: compactness_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: concavity_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: concave points_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: symmetry_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: fractal_dimension_worst\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n",
      "None\n",
      "\n",
      "Classification:\n",
      "\n",
      "KNN Classifier:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score (Positive)</th>\n",
       "      <th>F1 Score (Negative)</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV_5</th>\n",
       "      <td>0.9426</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>0.9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_10</th>\n",
       "      <td>0.9468</td>\n",
       "      <td>0.9729</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.9648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_15</th>\n",
       "      <td>0.9551</td>\n",
       "      <td>0.9744</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>0.9683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1 Score (Positive)  F1 Score (Negative)  AUC Score  Accuracy\n",
       "CV_5                0.9426               0.9665     0.9490    0.9578\n",
       "CV_10               0.9468               0.9729     0.9545    0.9648\n",
       "CV_15               0.9551               0.9744     0.9612    0.9683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score (Positive)</th>\n",
       "      <th>F1 Score (Negative)</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV_5</th>\n",
       "      <td>0.9646</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>0.9736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_10</th>\n",
       "      <td>0.9626</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>0.9718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_15</th>\n",
       "      <td>0.9706</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.9771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1 Score (Positive)  F1 Score (Negative)  AUC Score  Accuracy\n",
       "CV_5                0.9646               0.9789     0.9704    0.9736\n",
       "CV_10               0.9626               0.9768     0.9684    0.9718\n",
       "CV_15               0.9706               0.9806     0.9763    0.9771"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_cancer\n",
    "null_check(c_cancer)\n",
    "print(c_cancer.info())\n",
    "c_cancer = transform_data(c_cancer)\n",
    "\n",
    "# Define the number of splits for cross-validation\n",
    "cv_splits = [5, 10, 15]\n",
    "\n",
    "print(\"\\nClassification:\")\n",
    "knn_results = KNNScore(c_cancer, 'diagnosis', cv_splits)\n",
    "print(\"\\nKNN Classifier:\")\n",
    "display(knn_results)\n",
    "\n",
    "log_reg_results = LogisticRegressionScore(c_cancer, 'diagnosis', cv_splits)\n",
    "print(\"\\nLogistic Regression:\")\n",
    "display(log_reg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb2380c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:02<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 169, number of negative: 286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4693\n",
      "[LightGBM] [Info] Number of data points in the train set: 455, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.371429 -> initscore=-0.526093\n",
      "[LightGBM] [Info] Start training from score -0.526093\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 26/29 [00:02<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 195, number of negative: 317\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5296\n",
      "[LightGBM] [Info] Number of data points in the train set: 512, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380859 -> initscore=-0.485902\n",
      "[LightGBM] [Info] Start training from score -0.485902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  9.46it/s]\n",
      " 97%|█████████▋| 28/29 [00:03<00:00,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 199, number of negative: 332\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5494\n",
      "[LightGBM] [Info] Number of data points in the train set: 531, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374765 -> initscore=-0.511830\n",
      "[LightGBM] [Info] Start training from score -0.511830\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  8.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       Model                        \n",
       "CV_5   BernoulliNB                     0.9825\n",
       "       Perceptron                      0.9825\n",
       "       SVC                             0.9825\n",
       "       PassiveAggressiveClassifier     0.9737\n",
       "       ExtraTreesClassifier            0.9737\n",
       "       LogisticRegression              0.9737\n",
       "       SGDClassifier                   0.9649\n",
       "       CalibratedClassifierCV          0.9737\n",
       "       GaussianNB                      0.9649\n",
       "       RidgeClassifierCV               0.9649\n",
       "       RidgeClassifier                 0.9649\n",
       "       RandomForestClassifier          0.9649\n",
       "       LinearSVC                       0.9561\n",
       "       QuadraticDiscriminantAnalysis   0.9561\n",
       "       XGBClassifier                   0.9561\n",
       "       LGBMClassifier                  0.9561\n",
       "       BaggingClassifier               0.9561\n",
       "       NuSVC                           0.9561\n",
       "       NearestCentroid                 0.9561\n",
       "       LinearDiscriminantAnalysis      0.9561\n",
       "       KNeighborsClassifier            0.9474\n",
       "       AdaBoostClassifier              0.9474\n",
       "       DecisionTreeClassifier          0.9386\n",
       "       LabelSpreading                  0.9298\n",
       "       LabelPropagation                0.9298\n",
       "       ExtraTreeClassifier             0.9123\n",
       "       DummyClassifier                 0.6228\n",
       "CV_10  AdaBoostClassifier              0.9825\n",
       "       CalibratedClassifierCV          0.9825\n",
       "       Perceptron                      0.9825\n",
       "       LogisticRegression              0.9825\n",
       "       QuadraticDiscriminantAnalysis   0.9474\n",
       "       BaggingClassifier               0.9649\n",
       "       XGBClassifier                   0.9649\n",
       "       SVC                             0.9649\n",
       "       SGDClassifier                   0.9649\n",
       "       RidgeClassifierCV               0.9649\n",
       "       RidgeClassifier                 0.9649\n",
       "       RandomForestClassifier          0.9649\n",
       "       LinearSVC                       0.9649\n",
       "       LinearDiscriminantAnalysis      0.9649\n",
       "       GaussianNB                      0.9649\n",
       "       ExtraTreesClassifier            0.9649\n",
       "       LGBMClassifier                  0.9649\n",
       "       DecisionTreeClassifier          0.9474\n",
       "       PassiveAggressiveClassifier     0.9298\n",
       "       NearestCentroid                 0.9474\n",
       "       NuSVC                           0.9474\n",
       "       BernoulliNB                     0.9474\n",
       "       LabelSpreading                  0.9298\n",
       "       LabelPropagation                0.9298\n",
       "       KNeighborsClassifier            0.9298\n",
       "       ExtraTreeClassifier             0.8947\n",
       "       DummyClassifier                 0.7018\n",
       "CV_15  QuadraticDiscriminantAnalysis   0.9737\n",
       "       AdaBoostClassifier              0.9737\n",
       "       CalibratedClassifierCV          0.9737\n",
       "       SVC                             0.9737\n",
       "       Perceptron                      0.9737\n",
       "       LogisticRegression              0.9737\n",
       "       BaggingClassifier               0.9474\n",
       "       XGBClassifier                   0.9474\n",
       "       RidgeClassifierCV               0.9474\n",
       "       RidgeClassifier                 0.9474\n",
       "       RandomForestClassifier          0.9474\n",
       "       PassiveAggressiveClassifier     0.9474\n",
       "       LinearSVC                       0.9474\n",
       "       LinearDiscriminantAnalysis      0.9474\n",
       "       KNeighborsClassifier            0.9474\n",
       "       GaussianNB                      0.9474\n",
       "       ExtraTreesClassifier            0.9474\n",
       "       LGBMClassifier                  0.9474\n",
       "       SGDClassifier                   0.9474\n",
       "       DecisionTreeClassifier          0.9211\n",
       "       LabelSpreading                  0.9211\n",
       "       NearestCentroid                 0.9211\n",
       "       NuSVC                           0.9211\n",
       "       LabelPropagation                0.9211\n",
       "       BernoulliNB                     0.9211\n",
       "       ExtraTreeClassifier             0.8947\n",
       "       DummyClassifier                 0.6579\n",
       "Name: Accuracy, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using LazyPredict to check best model for our dataset\n",
    "LP_Cresults = LazyPredictClassification(c_cancer, 'diagnosis', [5, 10, 15])\n",
    "display(LP_Cresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536d768",
   "metadata": {},
   "source": [
    "## Dataset 2: Life Expectancy Dataset (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00297c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: Country\n",
      "Column DataType: object\n",
      "Column unique values: ['Afghanistan' 'Albania' 'Algeria' 'Angola' 'Antigua and Barbuda'\n",
      " 'Argentina' 'Armenia' 'Australia' 'Austria' 'Azerbaijan' 'Bahamas'\n",
      " 'Bahrain' 'Bangladesh' 'Barbados' 'Belarus' 'Belgium' 'Belize' 'Benin'\n",
      " 'Bhutan' 'Bolivia (Plurinational State of)' 'Bosnia and Herzegovina'\n",
      " 'Botswana' 'Brazil' 'Brunei Darussalam' 'Bulgaria' 'Burkina Faso'\n",
      " 'Burundi' \"Côte d'Ivoire\" 'Cabo Verde' 'Cambodia' 'Cameroon' 'Canada'\n",
      " 'Central African Republic' 'Chad' 'Chile' 'China' 'Colombia' 'Comoros'\n",
      " 'Congo' 'Cook Islands' 'Costa Rica' 'Croatia' 'Cuba' 'Cyprus' 'Czechia'\n",
      " \"Democratic People's Republic of Korea\"\n",
      " 'Democratic Republic of the Congo' 'Denmark' 'Djibouti' 'Dominica'\n",
      " 'Dominican Republic' 'Ecuador' 'Egypt' 'El Salvador' 'Equatorial Guinea'\n",
      " 'Eritrea' 'Estonia' 'Ethiopia' 'Fiji' 'Finland' 'France' 'Gabon' 'Gambia'\n",
      " 'Georgia' 'Germany' 'Ghana' 'Greece' 'Grenada' 'Guatemala' 'Guinea'\n",
      " 'Guinea-Bissau' 'Guyana' 'Haiti' 'Honduras' 'Hungary' 'Iceland' 'India'\n",
      " 'Indonesia' 'Iran (Islamic Republic of)' 'Iraq' 'Ireland' 'Israel'\n",
      " 'Italy' 'Jamaica' 'Japan' 'Jordan' 'Kazakhstan' 'Kenya' 'Kiribati'\n",
      " 'Kuwait' 'Kyrgyzstan' \"Lao People's Democratic Republic\" 'Latvia'\n",
      " 'Lebanon' 'Lesotho' 'Liberia' 'Libya' 'Lithuania' 'Luxembourg'\n",
      " 'Madagascar' 'Malawi' 'Malaysia' 'Maldives' 'Mali' 'Malta'\n",
      " 'Marshall Islands' 'Mauritania' 'Mauritius' 'Mexico'\n",
      " 'Micronesia (Federated States of)' 'Monaco' 'Mongolia' 'Montenegro'\n",
      " 'Morocco' 'Mozambique' 'Myanmar' 'Namibia' 'Nauru' 'Nepal' 'Netherlands'\n",
      " 'New Zealand' 'Nicaragua' 'Niger' 'Nigeria' 'Niue' 'Norway' 'Oman'\n",
      " 'Pakistan' 'Palau' 'Panama' 'Papua New Guinea' 'Paraguay' 'Peru'\n",
      " 'Philippines' 'Poland' 'Portugal' 'Qatar' 'Republic of Korea'\n",
      " 'Republic of Moldova' 'Romania' 'Russian Federation' 'Rwanda'\n",
      " 'Saint Kitts and Nevis' 'Saint Lucia' 'Saint Vincent and the Grenadines'\n",
      " 'Samoa' 'San Marino' 'Sao Tome and Principe' 'Saudi Arabia' 'Senegal'\n",
      " 'Serbia' 'Seychelles' 'Sierra Leone' 'Singapore' 'Slovakia' 'Slovenia'\n",
      " 'Solomon Islands' 'Somalia' 'South Africa' 'South Sudan' 'Spain'\n",
      " 'Sri Lanka' 'Sudan' 'Suriname' 'Swaziland' 'Sweden' 'Switzerland'\n",
      " 'Syrian Arab Republic' 'Tajikistan' 'Thailand'\n",
      " 'The former Yugoslav republic of Macedonia' 'Timor-Leste' 'Togo' 'Tonga'\n",
      " 'Trinidad and Tobago' 'Tunisia' 'Turkey' 'Turkmenistan' 'Tuvalu' 'Uganda'\n",
      " 'Ukraine' 'United Arab Emirates'\n",
      " 'United Kingdom of Great Britain and Northern Ireland'\n",
      " 'United Republic of Tanzania' 'United States of America' 'Uruguay'\n",
      " 'Uzbekistan' 'Vanuatu' 'Venezuela (Bolivarian Republic of)' 'Viet Nam'\n",
      " 'Yemen' 'Zambia' 'Zimbabwe']\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Year\n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Status\n",
      "Column DataType: object\n",
      "Column unique values: ['Developing' 'Developed']\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Life expectancy \n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 10\n",
      "\n",
      "\n",
      "Column Name: Adult Mortality\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 10\n",
      "\n",
      "\n",
      "Column Name: infant deaths\n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Alcohol\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 194\n",
      "\n",
      "\n",
      "Column Name: percentage expenditure\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Hepatitis B\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 553\n",
      "\n",
      "\n",
      "Column Name: Measles \n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name:  BMI \n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 34\n",
      "\n",
      "\n",
      "Column Name: under-five deaths \n",
      "Column DataType: int64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: Polio\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 19\n",
      "\n",
      "\n",
      "Column Name: Total expenditure\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 226\n",
      "\n",
      "\n",
      "Column Name: Diphtheria \n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 19\n",
      "\n",
      "\n",
      "Column Name:  HIV/AIDS\n",
      "Column DataType: float64\n",
      "Column has null: False\n",
      "\n",
      "\n",
      "Column Name: GDP\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 448\n",
      "\n",
      "\n",
      "Column Name: Population\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 652\n",
      "\n",
      "\n",
      "Column Name:  thinness  1-19 years\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 34\n",
      "\n",
      "\n",
      "Column Name:  thinness 5-9 years\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 34\n",
      "\n",
      "\n",
      "Column Name: Income composition of resources\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 167\n",
      "\n",
      "\n",
      "Column Name: Schooling\n",
      "Column DataType: float64\n",
      "Column has null: True\n",
      "Column Null Count: 163\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    float64\n",
      " 1   diagnosis                569 non-null    int32  \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(31), int32(1)\n",
      "memory usage: 140.2 KB\n",
      "None\n",
      "\n",
      "Regression:\n",
      "\n",
      "KNN Regression Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>P-Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV_5</th>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>0.3341</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>1,375.1319</td>\n",
       "      <td>1,489.1971</td>\n",
       "      <td>0.2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_10</th>\n",
       "      <td>0.2169</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>1,546.2709</td>\n",
       "      <td>1,662.9273</td>\n",
       "      <td>0.2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_15</th>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.1031</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.8971</td>\n",
       "      <td>1,602.8807</td>\n",
       "      <td>1,720.3372</td>\n",
       "      <td>0.2058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE    MSE   RMSE  R-Squared        AIC        BIC  P-Values\n",
       "CV_5  0.2295 0.1129 0.3341     0.8873 1,375.1319 1,489.1971    0.2145\n",
       "CV_10 0.2169 0.1034 0.3179     0.8968 1,546.2709 1,662.9273    0.2083\n",
       "CV_15 0.2165 0.1031 0.3161     0.8971 1,602.8807 1,720.3372    0.2058"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OLS Regression Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>P-Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV_5</th>\n",
       "      <td>0.3157</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>0.4109</td>\n",
       "      <td>0.8289</td>\n",
       "      <td>1,375.1319</td>\n",
       "      <td>1,489.1971</td>\n",
       "      <td>0.2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_10</th>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.1668</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>1,546.2709</td>\n",
       "      <td>1,662.9273</td>\n",
       "      <td>0.2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV_15</th>\n",
       "      <td>0.3135</td>\n",
       "      <td>0.1666</td>\n",
       "      <td>0.4074</td>\n",
       "      <td>0.8299</td>\n",
       "      <td>1,602.8807</td>\n",
       "      <td>1,720.3372</td>\n",
       "      <td>0.2058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE    MSE   RMSE  R-Squared        AIC        BIC  P-Values\n",
       "CV_5  0.3157 0.1689 0.4109     0.8289 1,375.1319 1,489.1971    0.2145\n",
       "CV_10 0.3140 0.1668 0.4083     0.8300 1,546.2709 1,662.9273    0.2083\n",
       "CV_15 0.3135 0.1666 0.4074     0.8299 1,602.8807 1,720.3372    0.2058"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_life_expectancy\n",
    "null_check(r_life_expectancy)\n",
    "print(c_cancer.info())\n",
    "clean_data(r_life_expectancy,[])\n",
    "r_life_expectancy = transform_data(r_life_expectancy)\n",
    "\n",
    "# Define the number of splits for cross-validation\n",
    "cv_splits = [5, 10, 15]\n",
    "\n",
    "print(\"\\nRegression:\")\n",
    "knn_metrics = KNNRegressionMetrics(r_life_expectancy, 'Life expectancy ', cv_splits)\n",
    "print(\"\\nKNN Regression Metrics:\")\n",
    "display(knn_metrics)\n",
    "\n",
    "ols_metrics = OLSRegressionMetrics(r_life_expectancy, 'Life expectancy ', cv_splits)\n",
    "print(\"\\nOLS Regression Metrics:\")\n",
    "display(ols_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516cbe1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 41/42 [00:18<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3281\n",
      "[LightGBM] [Info] Number of data points in the train set: 1319, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.017590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:19<00:00,  2.21it/s]\n",
      " 98%|█████████▊| 41/42 [00:20<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3315\n",
      "[LightGBM] [Info] Number of data points in the train set: 1484, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.017679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:21<00:00,  1.98it/s]\n",
      "100%|██████████| 42/42 [00:20<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3328\n",
      "[LightGBM] [Info] Number of data points in the train set: 1539, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -0.010673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:20<00:00,  2.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R-Squared</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"39\" valign=\"top\">CV_5</th>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.9618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.2101</td>\n",
       "      <td>0.9520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingRegressor</th>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.9515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.9491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingRegressor</th>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.9433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.9369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.9350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeRegressor</th>\n",
       "      <td>0.2611</td>\n",
       "      <td>0.9258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVR</th>\n",
       "      <td>0.2671</td>\n",
       "      <td>0.9223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.2673</td>\n",
       "      <td>0.9222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.2729</td>\n",
       "      <td>0.9189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.2902</td>\n",
       "      <td>0.9083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.3276</td>\n",
       "      <td>0.8831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.3314</td>\n",
       "      <td>0.8804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KernelRidge</th>\n",
       "      <td>0.4092</td>\n",
       "      <td>0.8177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoCV</th>\n",
       "      <td>0.4098</td>\n",
       "      <td>0.8172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.8170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsCV</th>\n",
       "      <td>0.4102</td>\n",
       "      <td>0.8168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>0.4102</td>\n",
       "      <td>0.8168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeCV</th>\n",
       "      <td>0.4103</td>\n",
       "      <td>0.8167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsIC</th>\n",
       "      <td>0.4104</td>\n",
       "      <td>0.8166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransformedTargetRegressor</th>\n",
       "      <td>0.4104</td>\n",
       "      <td>0.8166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.4104</td>\n",
       "      <td>0.8166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BayesianRidge</th>\n",
       "      <td>0.4106</td>\n",
       "      <td>0.8164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HuberRegressor</th>\n",
       "      <td>0.4108</td>\n",
       "      <td>0.8162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>0.4129</td>\n",
       "      <td>0.8144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDRegressor</th>\n",
       "      <td>0.4156</td>\n",
       "      <td>0.8119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuitCV</th>\n",
       "      <td>0.4358</td>\n",
       "      <td>0.7932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LarsCV</th>\n",
       "      <td>0.4529</td>\n",
       "      <td>0.7767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessRegressor</th>\n",
       "      <td>0.4850</td>\n",
       "      <td>0.7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweedieRegressor</th>\n",
       "      <td>0.4871</td>\n",
       "      <td>0.7416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuit</th>\n",
       "      <td>0.5373</td>\n",
       "      <td>0.6856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.5782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.7960</td>\n",
       "      <td>0.3101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyRegressor</th>\n",
       "      <td>0.9623</td>\n",
       "      <td>-0.0084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.9623</td>\n",
       "      <td>-0.0084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLars</th>\n",
       "      <td>0.9623</td>\n",
       "      <td>-0.0084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANSACRegressor</th>\n",
       "      <td>1.0990</td>\n",
       "      <td>-0.3151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lars</th>\n",
       "      <td>142.0047</td>\n",
       "      <td>-21,957.3028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"39\" valign=\"top\">CV_10</th>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.9706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingRegressor</th>\n",
       "      <td>0.1907</td>\n",
       "      <td>0.9579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.1990</td>\n",
       "      <td>0.9541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.2043</td>\n",
       "      <td>0.9516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingRegressor</th>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.9475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.9393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.9334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.2568</td>\n",
       "      <td>0.9236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVR</th>\n",
       "      <td>0.2568</td>\n",
       "      <td>0.9236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.9232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.2658</td>\n",
       "      <td>0.9182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeRegressor</th>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.3479</td>\n",
       "      <td>0.8598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HuberRegressor</th>\n",
       "      <td>0.4022</td>\n",
       "      <td>0.8126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LarsCV</th>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.8121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KernelRidge</th>\n",
       "      <td>0.4030</td>\n",
       "      <td>0.8119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>0.4031</td>\n",
       "      <td>0.8118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>0.4035</td>\n",
       "      <td>0.8114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BayesianRidge</th>\n",
       "      <td>0.4035</td>\n",
       "      <td>0.8114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoCV</th>\n",
       "      <td>0.4037</td>\n",
       "      <td>0.8112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.4041</td>\n",
       "      <td>0.8108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsCV</th>\n",
       "      <td>0.4055</td>\n",
       "      <td>0.8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeCV</th>\n",
       "      <td>0.4058</td>\n",
       "      <td>0.8093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.4061</td>\n",
       "      <td>0.8090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransformedTargetRegressor</th>\n",
       "      <td>0.4061</td>\n",
       "      <td>0.8090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsIC</th>\n",
       "      <td>0.4061</td>\n",
       "      <td>0.8090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDRegressor</th>\n",
       "      <td>0.4065</td>\n",
       "      <td>0.8086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessRegressor</th>\n",
       "      <td>0.4191</td>\n",
       "      <td>0.7965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuitCV</th>\n",
       "      <td>0.4219</td>\n",
       "      <td>0.7938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lars</th>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.7767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweedieRegressor</th>\n",
       "      <td>0.4744</td>\n",
       "      <td>0.7393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuit</th>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.6734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "      <td>0.6243</td>\n",
       "      <td>0.5485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.7768</td>\n",
       "      <td>0.3010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANSACRegressor</th>\n",
       "      <td>0.9246</td>\n",
       "      <td>0.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyRegressor</th>\n",
       "      <td>0.9458</td>\n",
       "      <td>-0.0362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.9458</td>\n",
       "      <td>-0.0362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLars</th>\n",
       "      <td>0.9458</td>\n",
       "      <td>-0.0362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"39\" valign=\"top\">CV_15</th>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.1733</td>\n",
       "      <td>0.9658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingRegressor</th>\n",
       "      <td>0.2035</td>\n",
       "      <td>0.9528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingRegressor</th>\n",
       "      <td>0.2066</td>\n",
       "      <td>0.9513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.9488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.9439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.9319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>0.2514</td>\n",
       "      <td>0.9279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.2589</td>\n",
       "      <td>0.9235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVR</th>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.9177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.2689</td>\n",
       "      <td>0.9175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.2785</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeRegressor</th>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.8883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.3466</td>\n",
       "      <td>0.8629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessRegressor</th>\n",
       "      <td>0.3809</td>\n",
       "      <td>0.8345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>0.4090</td>\n",
       "      <td>0.8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>0.4093</td>\n",
       "      <td>0.8089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDRegressor</th>\n",
       "      <td>0.4093</td>\n",
       "      <td>0.8089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BayesianRidge</th>\n",
       "      <td>0.4103</td>\n",
       "      <td>0.8080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HuberRegressor</th>\n",
       "      <td>0.4104</td>\n",
       "      <td>0.8079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoCV</th>\n",
       "      <td>0.4110</td>\n",
       "      <td>0.8073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KernelRidge</th>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.8069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.4118</td>\n",
       "      <td>0.8066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuitCV</th>\n",
       "      <td>0.4131</td>\n",
       "      <td>0.8054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeCV</th>\n",
       "      <td>0.4147</td>\n",
       "      <td>0.8038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsCV</th>\n",
       "      <td>0.4147</td>\n",
       "      <td>0.8038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsIC</th>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.8034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.8034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransformedTargetRegressor</th>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.8034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LarsCV</th>\n",
       "      <td>0.4357</td>\n",
       "      <td>0.7835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lars</th>\n",
       "      <td>0.4458</td>\n",
       "      <td>0.7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweedieRegressor</th>\n",
       "      <td>0.4638</td>\n",
       "      <td>0.7547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "      <td>0.4881</td>\n",
       "      <td>0.7283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuit</th>\n",
       "      <td>0.5280</td>\n",
       "      <td>0.6820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.7764</td>\n",
       "      <td>0.3124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyRegressor</th>\n",
       "      <td>0.9499</td>\n",
       "      <td>-0.0292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.9499</td>\n",
       "      <td>-0.0292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLars</th>\n",
       "      <td>0.9499</td>\n",
       "      <td>-0.0292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANSACRegressor</th>\n",
       "      <td>1.1517</td>\n",
       "      <td>-0.5130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        RMSE    R-Squared\n",
       "      Model                                              \n",
       "CV_5  ExtraTreesRegressor             0.1873       0.9618\n",
       "      LGBMRegressor                   0.2101       0.9520\n",
       "      HistGradientBoostingRegressor   0.2110       0.9515\n",
       "      RandomForestRegressor           0.2162       0.9491\n",
       "      BaggingRegressor                0.2281       0.9433\n",
       "      XGBRegressor                    0.2407       0.9369\n",
       "      GradientBoostingRegressor       0.2444       0.9350\n",
       "      ExtraTreeRegressor              0.2611       0.9258\n",
       "      NuSVR                           0.2671       0.9223\n",
       "      SVR                             0.2673       0.9222\n",
       "      MLPRegressor                    0.2729       0.9189\n",
       "      KNeighborsRegressor             0.2902       0.9083\n",
       "      AdaBoostRegressor               0.3276       0.8831\n",
       "      DecisionTreeRegressor           0.3314       0.8804\n",
       "      KernelRidge                     0.4092       0.8177\n",
       "      LassoCV                         0.4098       0.8172\n",
       "      Ridge                           0.4100       0.8170\n",
       "      LassoLarsCV                     0.4102       0.8168\n",
       "      ElasticNetCV                    0.4102       0.8168\n",
       "      RidgeCV                         0.4103       0.8167\n",
       "      LassoLarsIC                     0.4104       0.8166\n",
       "      TransformedTargetRegressor      0.4104       0.8166\n",
       "      LinearRegression                0.4104       0.8166\n",
       "      BayesianRidge                   0.4106       0.8164\n",
       "      HuberRegressor                  0.4108       0.8162\n",
       "      LinearSVR                       0.4129       0.8144\n",
       "      SGDRegressor                    0.4156       0.8119\n",
       "      OrthogonalMatchingPursuitCV     0.4358       0.7932\n",
       "      LarsCV                          0.4529       0.7767\n",
       "      GaussianProcessRegressor        0.4850       0.7439\n",
       "      TweedieRegressor                0.4871       0.7416\n",
       "      OrthogonalMatchingPursuit       0.5373       0.6856\n",
       "      PassiveAggressiveRegressor      0.6224       0.5782\n",
       "      ElasticNet                      0.7960       0.3101\n",
       "      DummyRegressor                  0.9623      -0.0084\n",
       "      Lasso                           0.9623      -0.0084\n",
       "      LassoLars                       0.9623      -0.0084\n",
       "      RANSACRegressor                 1.0990      -0.3151\n",
       "      Lars                          142.0047 -21,957.3028\n",
       "CV_10 ExtraTreesRegressor             0.1592       0.9706\n",
       "      LGBMRegressor                   0.1902       0.9581\n",
       "      HistGradientBoostingRegressor   0.1907       0.9579\n",
       "      RandomForestRegressor           0.1990       0.9541\n",
       "      XGBRegressor                    0.2043       0.9516\n",
       "      BaggingRegressor                0.2130       0.9475\n",
       "      MLPRegressor                    0.2290       0.9393\n",
       "      GradientBoostingRegressor       0.2397       0.9334\n",
       "      DecisionTreeRegressor           0.2568       0.9236\n",
       "      NuSVR                           0.2568       0.9236\n",
       "      SVR                             0.2575       0.9232\n",
       "      KNeighborsRegressor             0.2658       0.9182\n",
       "      ExtraTreeRegressor              0.3110       0.8880\n",
       "      AdaBoostRegressor               0.3479       0.8598\n",
       "      HuberRegressor                  0.4022       0.8126\n",
       "      LarsCV                          0.4027       0.8121\n",
       "      KernelRidge                     0.4030       0.8119\n",
       "      ElasticNetCV                    0.4031       0.8118\n",
       "      LinearSVR                       0.4035       0.8114\n",
       "      BayesianRidge                   0.4035       0.8114\n",
       "      LassoCV                         0.4037       0.8112\n",
       "      Ridge                           0.4041       0.8108\n",
       "      LassoLarsCV                     0.4055       0.8096\n",
       "      RidgeCV                         0.4058       0.8093\n",
       "      LinearRegression                0.4061       0.8090\n",
       "      TransformedTargetRegressor      0.4061       0.8090\n",
       "      LassoLarsIC                     0.4061       0.8090\n",
       "      SGDRegressor                    0.4065       0.8086\n",
       "      GaussianProcessRegressor        0.4191       0.7965\n",
       "      OrthogonalMatchingPursuitCV     0.4219       0.7938\n",
       "      Lars                            0.4391       0.7767\n",
       "      TweedieRegressor                0.4744       0.7393\n",
       "      OrthogonalMatchingPursuit       0.5310       0.6734\n",
       "      PassiveAggressiveRegressor      0.6243       0.5485\n",
       "      ElasticNet                      0.7768       0.3010\n",
       "      RANSACRegressor                 0.9246       0.0097\n",
       "      DummyRegressor                  0.9458      -0.0362\n",
       "      Lasso                           0.9458      -0.0362\n",
       "      LassoLars                       0.9458      -0.0362\n",
       "CV_15 ExtraTreesRegressor             0.1733       0.9658\n",
       "      LGBMRegressor                   0.2017       0.9536\n",
       "      BaggingRegressor                0.2035       0.9528\n",
       "      HistGradientBoostingRegressor   0.2066       0.9513\n",
       "      RandomForestRegressor           0.2119       0.9488\n",
       "      XGBRegressor                    0.2218       0.9439\n",
       "      MLPRegressor                    0.2444       0.9319\n",
       "      DecisionTreeRegressor           0.2514       0.9279\n",
       "      GradientBoostingRegressor       0.2589       0.9235\n",
       "      NuSVR                           0.2686       0.9177\n",
       "      SVR                             0.2689       0.9175\n",
       "      KNeighborsRegressor             0.2785       0.9115\n",
       "      ExtraTreeRegressor              0.3130       0.8883\n",
       "      AdaBoostRegressor               0.3466       0.8629\n",
       "      GaussianProcessRegressor        0.3809       0.8345\n",
       "      ElasticNetCV                    0.4090       0.8092\n",
       "      LinearSVR                       0.4093       0.8089\n",
       "      SGDRegressor                    0.4093       0.8089\n",
       "      BayesianRidge                   0.4103       0.8080\n",
       "      HuberRegressor                  0.4104       0.8079\n",
       "      LassoCV                         0.4110       0.8073\n",
       "      KernelRidge                     0.4115       0.8069\n",
       "      Ridge                           0.4118       0.8066\n",
       "      OrthogonalMatchingPursuitCV     0.4131       0.8054\n",
       "      RidgeCV                         0.4147       0.8038\n",
       "      LassoLarsCV                     0.4147       0.8038\n",
       "      LassoLarsIC                     0.4152       0.8034\n",
       "      LinearRegression                0.4152       0.8034\n",
       "      TransformedTargetRegressor      0.4152       0.8034\n",
       "      LarsCV                          0.4357       0.7835\n",
       "      Lars                            0.4458       0.7733\n",
       "      TweedieRegressor                0.4638       0.7547\n",
       "      PassiveAggressiveRegressor      0.4881       0.7283\n",
       "      OrthogonalMatchingPursuit       0.5280       0.6820\n",
       "      ElasticNet                      0.7764       0.3124\n",
       "      DummyRegressor                  0.9499      -0.0292\n",
       "      Lasso                           0.9499      -0.0292\n",
       "      LassoLars                       0.9499      -0.0292\n",
       "      RANSACRegressor                 1.1517      -0.5130"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking best model on basis of R-Squared and RMSE values using LazyPredict\n",
    "LP_Rresults = LazyPredictRegression(r_life_expectancy, 'Life expectancy ', [5,10,15])\n",
    "display(LP_Rresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c648232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
